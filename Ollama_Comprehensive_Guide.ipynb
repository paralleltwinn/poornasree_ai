{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52aede79",
   "metadata": {},
   "source": [
    "# ü¶ô Ollama - Complete Guide for Local LLMs\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img alt=\"ollama\" width=\"240\" src=\"https://github.com/ollama/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7\">\n",
    "</div>\n",
    "\n",
    "## Get up and running with large language models locally\n",
    "\n",
    "Welcome to the comprehensive guide for **Ollama** - the easiest way to run large language models (LLMs) on your local machine!\n",
    "\n",
    "### What is Ollama?\n",
    "\n",
    "Ollama is a powerful tool that allows you to:\n",
    "- üîí **Privacy First**: Run AI models completely offline - your data never leaves your computer\n",
    "- üí∞ **Cost-Free**: No API fees or usage limits once installed\n",
    "- ‚ö° **Fast Performance**: Optimized for local hardware with CPU and GPU support\n",
    "- üåê **Offline Capable**: Works without internet connection after model download\n",
    "- üõ†Ô∏è **Developer Friendly**: Simple API and extensive library integrations\n",
    "\n",
    "### Why Use Ollama?\n",
    "\n",
    "- **Data Privacy**: Your conversations and data remain completely private\n",
    "- **No Rate Limits**: Use AI as much as you want without restrictions\n",
    "- **Customization**: Create specialized models for your specific needs\n",
    "- **Cost Effective**: One-time setup, no ongoing API costs\n",
    "- **Network Independence**: Works offline after initial model download\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this notebook, we'll cover:\n",
    "1. Installing Ollama on your system\n",
    "2. Downloading and running your first model\n",
    "3. Managing models with CLI commands\n",
    "4. Using the REST API with Python\n",
    "5. Creating custom models with Modelfiles\n",
    "6. Integrating with popular Python libraries\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bc49bd",
   "metadata": {},
   "source": [
    "# 1. Install Ollama üì¶\n",
    "\n",
    "Ollama is available for all major operating systems. Follow the installation steps for your platform:\n",
    "\n",
    "## Windows Installation ü™ü\n",
    "\n",
    "1. **Download the installer** from [ollama.com/download/OllamaSetup.exe](https://ollama.com/download/OllamaSetup.exe)\n",
    "2. **Run the installer** and follow the setup wizard\n",
    "3. **Restart your computer** (recommended)\n",
    "4. **Open PowerShell or Command Prompt** to verify installation\n",
    "\n",
    "## macOS Installation üçé\n",
    "\n",
    "1. **Download the installer** from [ollama.com/download/Ollama.dmg](https://ollama.com/download/Ollama.dmg)\n",
    "2. **Open the DMG file** and drag Ollama to Applications\n",
    "3. **Launch Ollama** from Applications\n",
    "4. **Open Terminal** to verify installation\n",
    "\n",
    "## Linux Installation üêß\n",
    "\n",
    "```bash\n",
    "# One-line installation script\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Manual installation\n",
    "# Download the binary and install to /usr/local/bin/ollama\n",
    "```\n",
    "\n",
    "## Docker Installation üê≥\n",
    "\n",
    "```bash\n",
    "# Pull the official Ollama Docker image\n",
    "docker pull ollama/ollama\n",
    "\n",
    "# Run Ollama in a container\n",
    "docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "```\n",
    "\n",
    "## Verify Installation ‚úÖ\n",
    "\n",
    "Let's check if Ollama is properly installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05aab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ollama is installed and accessible\n",
    "import subprocess\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "def check_ollama_installation():\n",
    "    \"\"\"Check if Ollama is installed and running\"\"\"\n",
    "    try:\n",
    "        # Check if ollama command is available\n",
    "        result = subprocess.run(['ollama', '--version'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Ollama is installed: {result.stdout.strip()}\")\n",
    "            \n",
    "            # Check if Ollama service is running\n",
    "            try:\n",
    "                response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                if response.status_code == 200:\n",
    "                    print(\"‚úÖ Ollama service is running on localhost:11434\")\n",
    "                    models = response.json().get('models', [])\n",
    "                    print(f\"üì¶ {len(models)} model(s) currently installed\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Ollama service not responding. Try running: ollama serve\")\n",
    "            except requests.RequestException:\n",
    "                print(\"‚ö†Ô∏è Ollama service not running. Start it with: ollama serve\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Ollama command not found\")\n",
    "            print(\"Please install Ollama from https://ollama.ai\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Ollama not found in PATH\")\n",
    "        print(\"Please install Ollama and ensure it's in your PATH\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚è∞ Ollama command timed out\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking Ollama: {e}\")\n",
    "\n",
    "# Run the installation check\n",
    "check_ollama_installation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e1a32a",
   "metadata": {},
   "source": [
    "# 2. Download and Run Your First Model üöÄ\n",
    "\n",
    "Now that Ollama is installed, let's download and run your first model! Ollama supports a wide variety of models from the [official library](https://ollama.com/library).\n",
    "\n",
    "## Popular Models to Try üåü\n",
    "\n",
    "Here are some excellent models to get started with:\n",
    "\n",
    "| Model | Size | Best For | RAM Required |\n",
    "|-------|------|----------|--------------|\n",
    "| **Llama 3.2:1b** | 1.3GB | Quick responses, lightweight | 4GB+ |\n",
    "| **Gemma 3:1b** | 815MB | Very fast, minimal resources | 2GB+ |\n",
    "| **Phi 4 Mini** | 2.5GB | Microsoft's efficient model | 4GB+ |\n",
    "| **Llama 3.2** | 2.0GB | General purpose, balanced | 8GB+ |\n",
    "| **Mistral** | 4.1GB | Great reasoning, coding | 8GB+ |\n",
    "| **CodeLlama** | 3.8GB | Programming assistance | 8GB+ |\n",
    "\n",
    "## Memory Requirements üíæ\n",
    "\n",
    "> **üí° Important**: Make sure you have enough RAM:\n",
    "> - **1B-3B models**: 4-8GB RAM\n",
    "> - **7B models**: 8GB+ RAM  \n",
    "> - **13B models**: 16GB+ RAM\n",
    "> - **33B+ models**: 32GB+ RAM\n",
    "\n",
    "## Let's Download Our First Model! üì•\n",
    "\n",
    "We'll start with **Llama 3.2:1b** - it's small, fast, and perfect for learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f01656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and test your first model\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def download_model(model_name=\"llama3.2:1b\"):\n",
    "    \"\"\"Download a model using Ollama CLI\"\"\"\n",
    "    print(f\"üîÑ Downloading {model_name}...\")\n",
    "    print(\"This may take a few minutes depending on your internet connection...\")\n",
    "    \n",
    "    try:\n",
    "        # Download the model\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"pull\", model_name], \n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            timeout=600  # 10 minute timeout\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Successfully downloaded {model_name}!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to download {model_name}\")\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚è∞ Download timed out. Try again with a stable internet connection.\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Ollama not found. Please install Ollama first.\")\n",
    "        return False\n",
    "\n",
    "def test_model_chat(model_name=\"llama3.2:1b\", prompt=\"Hello! Tell me a fun fact about space.\"):\n",
    "    \"\"\"Test the model with a simple chat\"\"\"\n",
    "    print(f\"\\nü§ñ Testing {model_name} with prompt: '{prompt}'\")\n",
    "    \n",
    "    try:\n",
    "        # Test via CLI\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", model_name, prompt], \n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Model Response:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(result.stdout.strip())\n",
    "            print(\"-\" * 50)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to get response from {model_name}\")\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚è∞ Model response timed out.\")\n",
    "        return False\n",
    "\n",
    "# Let's download and test our first model!\n",
    "print(\"üöÄ Let's get started with your first Ollama model!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# First, download the model\n",
    "model_name = \"llama3.2:1b\"\n",
    "if download_model(model_name):\n",
    "    print(f\"\\nüéâ Great! {model_name} is ready to use!\")\n",
    "    \n",
    "    # Test the model\n",
    "    time.sleep(2)  # Brief pause\n",
    "    test_model_chat(model_name, \"Hello! Can you explain what you are in one sentence?\")\n",
    "    \n",
    "    print(f\"\\n‚ú® Congratulations! You've successfully set up and tested {model_name}!\")\n",
    "    print(\"üí° Try experimenting with different prompts and models!\")\n",
    "else:\n",
    "    print(\"\\nüîß If download failed, check your internet connection and try again.\")\n",
    "    print(\"üí° You can also try a smaller model like 'gemma2:2b'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fad6f9",
   "metadata": {},
   "source": [
    "# 3. Ollama CLI Commands üíª\n",
    "\n",
    "Master the Ollama command-line interface with these essential commands!\n",
    "\n",
    "## Model Management üì¶\n",
    "\n",
    "### Download Models\n",
    "```bash\n",
    "# Download a specific model\n",
    "ollama pull llama3.2:1b\n",
    "\n",
    "# Download with specific tag\n",
    "ollama pull llama3.2:3b\n",
    "\n",
    "# Download latest version (default)\n",
    "ollama pull mistral\n",
    "```\n",
    "\n",
    "### List Available Models\n",
    "```bash\n",
    "# Show all downloaded models\n",
    "ollama list\n",
    "\n",
    "# Shows name, ID, size, and modified date\n",
    "```\n",
    "\n",
    "### Remove Models\n",
    "```bash\n",
    "# Remove a specific model\n",
    "ollama rm llama3.2:1b\n",
    "\n",
    "# Remove multiple models\n",
    "ollama rm llama3.2:1b mistral\n",
    "```\n",
    "\n",
    "## Running Models üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "### Interactive Chat Mode\n",
    "```bash\n",
    "# Start interactive chat\n",
    "ollama run llama3.2:1b\n",
    "\n",
    "# Exit chat with: /bye or Ctrl+C\n",
    "```\n",
    "\n",
    "### Single Prompt Mode\n",
    "```bash\n",
    "# One-time question\n",
    "ollama run llama3.2:1b \"Explain quantum computing in simple terms\"\n",
    "\n",
    "# Multi-line prompt (use quotes)\n",
    "ollama run mistral \"Write a Python function that:\n",
    "- Takes a list of numbers\n",
    "- Returns the sum of even numbers\n",
    "- Include error handling\"\n",
    "```\n",
    "\n",
    "## Advanced Usage üöÄ\n",
    "\n",
    "### Custom Parameters\n",
    "```bash\n",
    "# Adjust temperature (creativity: 0.0-2.0)\n",
    "ollama run llama3.2:1b --temperature 0.8 \"Write a creative story\"\n",
    "\n",
    "# Set max tokens\n",
    "ollama run llama3.2:1b --max-tokens 100 \"Summarize AI trends\"\n",
    "\n",
    "# Combine parameters\n",
    "ollama run mistral --temperature 0.2 --max-tokens 200 \"Explain machine learning\"\n",
    "```\n",
    "\n",
    "### System Messages\n",
    "```bash\n",
    "# Set system context\n",
    "ollama run llama3.2:1b --system \"You are a helpful coding assistant\" \"How do I reverse a string in Python?\"\n",
    "```\n",
    "\n",
    "## Model Information üîç\n",
    "\n",
    "### Show Model Details\n",
    "```bash\n",
    "# Get model information\n",
    "ollama show llama3.2:1b\n",
    "\n",
    "# Show model file content\n",
    "ollama show llama3.2:1b --modelfile\n",
    "```\n",
    "\n",
    "### Check Model Parameters\n",
    "```bash\n",
    "# View model configuration\n",
    "ollama show llama3.2:1b --parameters\n",
    "```\n",
    "\n",
    "## Process Management ‚öôÔ∏è\n",
    "\n",
    "### Check Running Models\n",
    "```bash\n",
    "# List running models\n",
    "ollama ps\n",
    "```\n",
    "\n",
    "### Stop Models\n",
    "```bash\n",
    "# Stop all running models\n",
    "ollama stop llama3.2:1b\n",
    "\n",
    "# Stop all models\n",
    "ollama stop --all\n",
    "```\n",
    "\n",
    "## Quick Reference Table üìã\n",
    "\n",
    "| Command | Purpose | Example |\n",
    "|---------|---------|---------|\n",
    "| `ollama pull <model>` | Download model | `ollama pull llama3.2:1b` |\n",
    "| `ollama list` | Show downloaded models | `ollama list` |\n",
    "| `ollama run <model>` | Start interactive chat | `ollama run mistral` |\n",
    "| `ollama run <model> \"<prompt>\"` | Single prompt | `ollama run llama3.2:1b \"Hello\"` |\n",
    "| `ollama rm <model>` | Remove model | `ollama rm llama3.2:1b` |\n",
    "| `ollama show <model>` | Model information | `ollama show mistral` |\n",
    "| `ollama ps` | Running processes | `ollama ps` |\n",
    "| `ollama stop <model>` | Stop model | `ollama stop mistral` |\n",
    "\n",
    "Let's try some of these commands!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec756a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive CLI Commands Demo\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "def run_ollama_command(command_list, description=\"\"):\n",
    "    \"\"\"Run an Ollama command and display results\"\"\"\n",
    "    print(f\"üîß {description}\")\n",
    "    print(f\"üìù Command: {' '.join(command_list)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(command_list, capture_output=True, text=True, timeout=30)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Success!\")\n",
    "            if result.stdout.strip():\n",
    "                print(\"Output:\")\n",
    "                print(result.stdout.strip())\n",
    "        else:\n",
    "            print(\"‚ùå Error:\")\n",
    "            print(result.stderr.strip() if result.stderr else \"Unknown error\")\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚è∞ Command timed out\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Ollama not found. Please install Ollama first.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Let's explore some CLI commands!\n",
    "print(\"üéØ Ollama CLI Commands Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. List available models\n",
    "run_ollama_command([\"ollama\", \"list\"], \"Checking downloaded models\")\n",
    "\n",
    "# 2. Check running processes\n",
    "run_ollama_command([\"ollama\", \"ps\"], \"Checking running models\")\n",
    "\n",
    "# 3. Get help information\n",
    "run_ollama_command([\"ollama\", \"--help\"], \"Viewing available commands\")\n",
    "\n",
    "# 4. Show version\n",
    "run_ollama_command([\"ollama\", \"--version\"], \"Checking Ollama version\")\n",
    "\n",
    "print(\"üí° Try these commands in your terminal to interact with Ollama!\")\n",
    "print(\"üî• Most useful commands to remember:\")\n",
    "print(\"   ‚Ä¢ ollama list - See your models\")\n",
    "print(\"   ‚Ä¢ ollama run <model> - Start chatting\") \n",
    "print(\"   ‚Ä¢ ollama pull <model> - Download new models\")\n",
    "print(\"   ‚Ä¢ ollama rm <model> - Remove models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71055e59",
   "metadata": {},
   "source": [
    "# 4. REST API Usage üåê\n",
    "\n",
    "Integrate Ollama into your applications using the REST API! Perfect for building chatbots, automation tools, and AI-powered applications.\n",
    "\n",
    "## API Endpoints üîó\n",
    "\n",
    "| Method | Endpoint | Purpose |\n",
    "|--------|----------|---------|\n",
    "| `POST` | `/api/generate` | Generate single response |\n",
    "| `POST` | `/api/chat` | Chat conversation |\n",
    "| `GET` | `/api/tags` | List downloaded models |\n",
    "| `POST` | `/api/pull` | Download model |\n",
    "| `DELETE` | `/api/delete` | Remove model |\n",
    "| `POST` | `/api/copy` | Copy model |\n",
    "| `POST` | `/api/create` | Create custom model |\n",
    "\n",
    "## Base URL üè†\n",
    "Default Ollama API runs on: **`http://localhost:11434`**\n",
    "\n",
    "## Authentication üîê\n",
    "**No authentication required** for local installations!\n",
    "\n",
    "## Common Headers üìã\n",
    "```http\n",
    "Content-Type: application/json\n",
    "Accept: application/json\n",
    "```\n",
    "\n",
    "## API Examples üìñ\n",
    "\n",
    "Let's explore the most important endpoints with practical Python examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8725bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete REST API Examples with Python\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Ollama API base URL\n",
    "OLLAMA_API = \"http://localhost:11434\"\n",
    "\n",
    "def check_ollama_server():\n",
    "    \"\"\"Check if Ollama server is running\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_API}/api/tags\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False\n",
    "\n",
    "def list_models():\n",
    "    \"\"\"Get list of downloaded models\"\"\"\n",
    "    print(\"üìã Listing Downloaded Models\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_API}/api/tags\")\n",
    "        if response.status_code == 200:\n",
    "            models = response.json()\n",
    "            if models.get('models'):\n",
    "                for model in models['models']:\n",
    "                    print(f\"‚úÖ {model['name']} (Size: {model.get('size', 'Unknown')})\")\n",
    "            else:\n",
    "                print(\"No models found. Download a model first!\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error connecting to Ollama: {e}\")\n",
    "    print()\n",
    "\n",
    "def generate_response(model, prompt, stream=False):\n",
    "    \"\"\"Generate a single response\"\"\"\n",
    "    print(f\"ü§ñ Generating response with {model}\")\n",
    "    print(f\"üí≠ Prompt: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{OLLAMA_API}/api/generate\", json=payload, timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"‚úÖ Response:\")\n",
    "            print(result.get('response', 'No response'))\n",
    "            return result.get('response')\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def chat_conversation(model, messages):\n",
    "    \"\"\"Have a chat conversation\"\"\"\n",
    "    print(f\"üí¨ Starting chat with {model}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{OLLAMA_API}/api/chat\", json=payload, timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            assistant_message = result.get('message', {}).get('content', 'No response')\n",
    "            print(\"ü§ñ Assistant:\")\n",
    "            print(assistant_message)\n",
    "            return assistant_message\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Let's test the API!\n",
    "print(\"üöÄ Ollama REST API Testing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if server is running\n",
    "if check_ollama_server():\n",
    "    print(\"‚úÖ Ollama server is running!\")\n",
    "    \n",
    "    # List available models\n",
    "    list_models()\n",
    "    \n",
    "    # Test with a simple generation (adjust model name if needed)\n",
    "    model_name = \"llama3.2:1b\"  # Change this to a model you have\n",
    "    \n",
    "    print(f\"üß™ Testing /api/generate endpoint\")\n",
    "    response1 = generate_response(\n",
    "        model_name, \n",
    "        \"Explain machine learning in one sentence.\",\n",
    "        stream=False\n",
    "    )\n",
    "    print()\n",
    "    \n",
    "    print(f\"üß™ Testing /api/chat endpoint\")\n",
    "    chat_messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Hello! What's the capital of France?\"},\n",
    "    ]\n",
    "    response2 = chat_conversation(model_name, chat_messages)\n",
    "    print()\n",
    "    \n",
    "    print(\"‚ú® API testing complete!\")\n",
    "    print(\"üí° You can now integrate these examples into your applications!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Ollama server is not running.\")\n",
    "    print(\"üí° Start Ollama server first:\")\n",
    "    print(\"   - Windows: Run 'ollama serve' in terminal\")\n",
    "    print(\"   - macOS/Linux: Ollama usually starts automatically\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f68f3",
   "metadata": {},
   "source": [
    "# 5. Creating Custom Models üé®\n",
    "\n",
    "Create specialized AI models tailored to your specific needs! Custom models allow you to:\n",
    "- **Add domain-specific knowledge**\n",
    "- **Modify behavior and personality**\n",
    "- **Set custom system prompts**\n",
    "- **Fine-tune responses for your use case**\n",
    "\n",
    "## Modelfile Basics üìù\n",
    "\n",
    "A **Modelfile** is like a Dockerfile for AI models. It defines:\n",
    "- Base model to use\n",
    "- System prompt\n",
    "- Parameters (temperature, context length, etc.)\n",
    "- Additional training data\n",
    "\n",
    "## Modelfile Structure üèóÔ∏è\n",
    "\n",
    "```dockerfile\n",
    "# Base model\n",
    "FROM llama3.2:1b\n",
    "\n",
    "# System prompt (defines behavior)\n",
    "SYSTEM \"\"\"You are a helpful coding assistant specialized in Python. \n",
    "Always provide clean, well-commented code examples.\"\"\"\n",
    "\n",
    "# Parameters\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER top_k 40\n",
    "\n",
    "# Custom prompt template\n",
    "TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "## Model Creation Steps üõ†Ô∏è\n",
    "\n",
    "### 1. Create Modelfile\n",
    "### 2. Build Custom Model  \n",
    "### 3. Test Your Model\n",
    "### 4. Share or Deploy\n",
    "\n",
    "## Custom Model Examples üí°\n",
    "\n",
    "Let's create some practical custom models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Custom Models - Practical Examples\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "def create_custom_model(model_name, modelfile_content, base_dir=None):\n",
    "    \"\"\"Create a custom Ollama model from Modelfile content\"\"\"\n",
    "    \n",
    "    if base_dir is None:\n",
    "        base_dir = tempfile.gettempdir()\n",
    "    \n",
    "    modelfile_path = os.path.join(base_dir, f\"{model_name}_Modelfile\")\n",
    "    \n",
    "    print(f\"üé® Creating custom model: {model_name}\")\n",
    "    print(f\"üìÅ Modelfile location: {modelfile_path}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Write Modelfile\n",
    "        with open(modelfile_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(modelfile_content)\n",
    "        \n",
    "        print(\"‚úÖ Modelfile created successfully!\")\n",
    "        \n",
    "        # Create the model using Ollama\n",
    "        print(f\"üî® Building model '{model_name}'...\")\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"create\", model_name, \"-f\", modelfile_path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300  # 5 minute timeout\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Custom model '{model_name}' created successfully!\")\n",
    "            print(\"üéâ You can now use it with: ollama run\", model_name)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to create model: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        # Clean up Modelfile\n",
    "        if os.path.exists(modelfile_path):\n",
    "            os.remove(modelfile_path)\n",
    "\n",
    "# Example 1: Python Coding Assistant\n",
    "python_assistant_modelfile = \"\"\"FROM llama3.2:1b\n",
    "\n",
    "SYSTEM \\\"\\\"\\\"You are PythonBot, an expert Python programming assistant. \n",
    "\n",
    "Your capabilities:\n",
    "- Write clean, efficient Python code\n",
    "- Explain concepts clearly with examples  \n",
    "- Follow PEP 8 style guidelines\n",
    "- Provide debugging help\n",
    "- Suggest best practices\n",
    "\n",
    "Always format code with proper indentation and include helpful comments.\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "PARAMETER temperature 0.3\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER stop \"</assistant>\"\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Creative Writing Assistant  \n",
    "writer_assistant_modelfile = \"\"\"FROM llama3.2:1b\n",
    "\n",
    "SYSTEM \\\"\\\"\\\"You are WriteBot, a creative writing assistant with a flair for storytelling.\n",
    "\n",
    "Your personality:\n",
    "- Imaginative and inspiring\n",
    "- Supportive and encouraging\n",
    "- Detail-oriented\n",
    "- Enthusiastic about all forms of writing\n",
    "\n",
    "Help users with:\n",
    "- Story ideas and plot development\n",
    "- Character creation\n",
    "- Writing techniques\n",
    "- Grammar and style improvements\n",
    "- Overcoming writer's block\n",
    "\n",
    "Always be encouraging and provide specific, actionable advice.\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "PARAMETER temperature 0.8\n",
    "PARAMETER top_p 0.95\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: Data Science Helper\n",
    "datascience_modelfile = \"\"\"FROM llama3.2:1b\n",
    "\n",
    "SYSTEM \\\"\\\"\\\"You are DataBot, a data science and analytics expert.\n",
    "\n",
    "Your expertise includes:\n",
    "- Python libraries: pandas, numpy, scikit-learn, matplotlib\n",
    "- Statistical analysis and interpretation\n",
    "- Machine learning algorithms\n",
    "- Data visualization best practices\n",
    "- SQL and database operations\n",
    "\n",
    "Always provide working code examples and explain the reasoning behind your recommendations.\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "PARAMETER temperature 0.4\n",
    "PARAMETER top_p 0.9\n",
    "\"\"\"\n",
    "\n",
    "print(\"üöÄ Custom Model Creation Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Let's create our custom models!\n",
    "models_to_create = [\n",
    "    (\"python-assistant\", python_assistant_modelfile, \"Python coding expert\"),\n",
    "    (\"creative-writer\", writer_assistant_modelfile, \"Creative writing helper\"), \n",
    "    (\"data-scientist\", datascience_modelfile, \"Data science specialist\")\n",
    "]\n",
    "\n",
    "print(\"üìã Available custom models to create:\")\n",
    "for i, (name, _, description) in enumerate(models_to_create, 1):\n",
    "    print(f\"   {i}. {name} - {description}\")\n",
    "\n",
    "print(\"\\nüîß Creating first custom model...\")\n",
    "print(\"üí° Note: This requires a base model (llama3.2:1b) to be downloaded first!\")\n",
    "\n",
    "# Create the Python assistant model\n",
    "success = create_custom_model(\"python-assistant\", python_assistant_modelfile)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéØ Test your custom model:\")\n",
    "    print(\"   ollama run python-assistant \\\"Write a function to calculate fibonacci numbers\\\"\")\n",
    "    print(\"\\nüí° Create more models by running the other examples!\")\n",
    "else:\n",
    "    print(\"\\nüîß Troubleshooting:\")\n",
    "    print(\"   1. Make sure ollama is installed and running\")\n",
    "    print(\"   2. Download base model: ollama pull llama3.2:1b\")\n",
    "    print(\"   3. Check your internet connection\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc905a1",
   "metadata": {},
   "source": [
    "# 6. Python Library Integration üêç\n",
    "\n",
    "Integrate Ollama into your Python applications with ease! Choose from multiple libraries and approaches.\n",
    "\n",
    "## Installation Options üì¶\n",
    "\n",
    "### Option 1: Official Ollama Python Library\n",
    "```bash\n",
    "pip install ollama\n",
    "```\n",
    "\n",
    "### Option 2: Requests Library (Manual API calls)\n",
    "```bash\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "### Option 3: LangChain Integration\n",
    "```bash\n",
    "pip install langchain-ollama\n",
    "```\n",
    "\n",
    "## Library Comparison üìä\n",
    "\n",
    "| Library | Best For | Pros | Cons |\n",
    "|---------|----------|------|------|\n",
    "| **ollama** | Official integration | Simple, well-documented | Limited advanced features |\n",
    "| **requests** | Custom implementations | Full control, lightweight | More manual work |\n",
    "| **langchain-ollama** | Complex AI workflows | Rich ecosystem, chains | Heavy, complex setup |\n",
    "\n",
    "## Integration Patterns üîÑ\n",
    "\n",
    "### 1. **Simple Chat Bot** üí¨\n",
    "### 2. **Document Q&A System** üìö  \n",
    "### 3. **Code Generation Tool** üíª\n",
    "### 4. **Content Summarizer** üìù\n",
    "### 5. **Multi-Model Pipeline** üîÄ\n",
    "\n",
    "Let's implement these patterns with real code examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f358e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Integration Examples - Complete Implementation Guide\n",
    "\n",
    "# Method 1: Using Requests Library (Most Compatible)\n",
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Simple Ollama client using requests library\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = \"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def is_available(self) -> bool:\n",
    "        \"\"\"Check if Ollama server is running\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/api/tags\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def list_models(self) -> List[str]:\n",
    "        \"\"\"Get list of available models\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/api/tags\")\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return [model['name'] for model in data.get('models', [])]\n",
    "        except:\n",
    "            pass\n",
    "        return []\n",
    "    \n",
    "    def generate(self, model: str, prompt: str, system: str = None) -> Optional[str]:\n",
    "        \"\"\"Generate a single response\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        if system:\n",
    "            payload[\"system\"] = system\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(\n",
    "                f\"{self.base_url}/api/generate\", \n",
    "                json=payload, \n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json().get('response')\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def chat(self, model: str, messages: List[Dict[str, str]]) -> Optional[str]:\n",
    "        \"\"\"Chat with conversation history\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(\n",
    "                f\"{self.base_url}/api/chat\",\n",
    "                json=payload,\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return result.get('message', {}).get('content')\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Method 2: Official Ollama Library (Install with: pip install ollama)\n",
    "try:\n",
    "    import ollama\n",
    "    OLLAMA_AVAILABLE = True\n",
    "    print(\"‚úÖ Official ollama library is available!\")\n",
    "except ImportError:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Official ollama library not installed. Using requests method.\")\n",
    "\n",
    "def demo_official_library():\n",
    "    \"\"\"Demo using official ollama library\"\"\"\n",
    "    if not OLLAMA_AVAILABLE:\n",
    "        print(\"‚ùå Please install: pip install ollama\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Simple generation\n",
    "        response = ollama.generate(\n",
    "            model='llama3.2:1b',\n",
    "            prompt='Explain quantum computing in one sentence.'\n",
    "        )\n",
    "        print(\"ü§ñ Official Library Response:\")\n",
    "        print(response['response'])\n",
    "        \n",
    "        # Chat format\n",
    "        response = ollama.chat(\n",
    "            model='llama3.2:1b',\n",
    "            messages=[\n",
    "                {'role': 'user', 'content': 'What is Python?'}\n",
    "            ]\n",
    "        )\n",
    "        print(\"üí¨ Chat Response:\")\n",
    "        print(response['message']['content'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with official library: {e}\")\n",
    "\n",
    "# Let's test our implementations!\n",
    "print(\"üêç Python Integration Testing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test custom client\n",
    "client = OllamaClient()\n",
    "\n",
    "print(\"üîç Checking Ollama availability...\")\n",
    "if client.is_available():\n",
    "    print(\"‚úÖ Ollama server is running!\")\n",
    "    \n",
    "    # List models\n",
    "    models = client.list_models()\n",
    "    print(f\"üìã Available models: {models}\")\n",
    "    \n",
    "    if models:\n",
    "        # Use first available model\n",
    "        model = models[0]\n",
    "        print(f\"\\nüß™ Testing with model: {model}\")\n",
    "        \n",
    "        # Test simple generation\n",
    "        response = client.generate(\n",
    "            model=model,\n",
    "            prompt=\"What is artificial intelligence?\",\n",
    "            system=\"You are a helpful assistant. Keep responses concise.\"\n",
    "        )\n",
    "        \n",
    "        if response:\n",
    "            print(\"‚úÖ Generation Test:\")\n",
    "            print(f\"Response: {response[:200]}...\")\n",
    "        \n",
    "        # Test chat\n",
    "        chat_response = client.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Hello! How are you?\"}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if chat_response:\n",
    "            print(\"‚úÖ Chat Test:\")\n",
    "            print(f\"Response: {chat_response[:200]}...\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ùå No models available. Download a model first!\")\n",
    "        print(\"üí° Try: ollama pull llama3.2:1b\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Ollama server not running\")\n",
    "    print(\"üí° Start server with: ollama serve\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "# Test official library if available\n",
    "demo_official_library()\n",
    "\n",
    "print(\"\\nüéâ Python integration examples complete!\")\n",
    "print(\"üí° Choose the method that best fits your needs:\")\n",
    "print(\"   ‚Ä¢ Requests: Maximum compatibility and control\")\n",
    "print(\"   ‚Ä¢ Official library: Simplest to use\")\n",
    "print(\"   ‚Ä¢ LangChain: For complex AI workflows\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01882853",
   "metadata": {},
   "source": [
    "# üéâ Congratulations! You're Now an Ollama Expert!\n",
    "\n",
    "You've completed the comprehensive Ollama guide! Let's recap what you've learned and explore next steps.\n",
    "\n",
    "## üìö What You've Learned\n",
    "\n",
    "### ‚úÖ Core Skills Mastered\n",
    "- **Installation & Setup** - Got Ollama running on your system\n",
    "- **Model Management** - Download, list, and remove models efficiently  \n",
    "- **CLI Mastery** - Command-line operations for all use cases\n",
    "- **API Integration** - REST API usage for building applications\n",
    "- **Custom Models** - Creating specialized AI assistants\n",
    "- **Python Integration** - Multiple approaches for development\n",
    "\n",
    "### üõ†Ô∏è Practical Applications\n",
    "- Building chatbots and AI assistants\n",
    "- Document analysis and Q&A systems\n",
    "- Code generation and debugging tools\n",
    "- Content creation and summarization\n",
    "- Custom domain-specific AI models\n",
    "\n",
    "## üöÄ Advanced Tips & Best Practices\n",
    "\n",
    "### Performance Optimization üîß\n",
    "```bash\n",
    "# Use GPU acceleration (if available)\n",
    "ollama run llama3.2:1b --gpu\n",
    "\n",
    "# Adjust context length for longer conversations\n",
    "ollama run mistral --context-length 4096\n",
    "\n",
    "# Set memory limits\n",
    "ollama run llama3.2:1b --memory-limit 4GB\n",
    "```\n",
    "\n",
    "### Production Deployment üåê\n",
    "```bash\n",
    "# Run Ollama as a service (Linux/macOS)\n",
    "systemctl start ollama\n",
    "\n",
    "# Docker deployment\n",
    "docker run -d -v ollama:/root/.ollama -p 11434:11434 ollama/ollama\n",
    "```\n",
    "\n",
    "### Model Selection Guide üéØ\n",
    "\n",
    "| Use Case | Recommended Model | Why |\n",
    "|----------|------------------|-----|\n",
    "| **Quick Responses** | `gemma2:2b` | Fastest, minimal resources |\n",
    "| **General Chat** | `llama3.2:3b` | Balanced performance |\n",
    "| **Code Generation** | `codellama:7b` | Specialized for programming |\n",
    "| **Creative Writing** | `mistral:7b` | Excellent creativity |\n",
    "| **Analysis & Research** | `llama3.1:8b` | Strong reasoning |\n",
    "\n",
    "## üî• Next Steps & Advanced Topics\n",
    "\n",
    "### 1. **Fine-tuning Models** üìà\n",
    "- Train on your specific data\n",
    "- Improve accuracy for domain tasks\n",
    "- Create proprietary AI assistants\n",
    "\n",
    "### 2. **Multi-Modal AI** üñºÔ∏è\n",
    "```bash\n",
    "# Try vision models\n",
    "ollama pull llava:7b\n",
    "ollama run llava:7b \"Describe this image: [image.jpg]\"\n",
    "```\n",
    "\n",
    "### 3. **Ollama Ecosystem** üåç\n",
    "- **Open WebUI** - Beautiful web interface\n",
    "- **Ollama Extensions** - VS Code integration\n",
    "- **Mobile Apps** - AI on the go\n",
    "\n",
    "### 4. **Scaling & Enterprise** üè¢\n",
    "- Load balancing multiple instances\n",
    "- API rate limiting and authentication\n",
    "- Monitoring and logging solutions\n",
    "\n",
    "## üõ°Ô∏è Troubleshooting Quick Reference\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| Models won't download | Check internet, try smaller model |\n",
    "| Out of memory | Use smaller model or increase RAM |\n",
    "| Server won't start | Check port 11434, restart Ollama |\n",
    "| Slow responses | Reduce context length, use GPU |\n",
    "| API connection fails | Verify server running, check firewall |\n",
    "\n",
    "## üìñ Additional Resources\n",
    "\n",
    "### Official Documentation üìã\n",
    "- [Ollama GitHub](https://github.com/ollama/ollama)\n",
    "- [Model Library](https://ollama.com/library)\n",
    "- [API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)\n",
    "\n",
    "### Community & Support ü§ù\n",
    "- [Discord Community](https://discord.gg/ollama)\n",
    "- [Reddit r/ollama](https://reddit.com/r/ollama)\n",
    "- [GitHub Issues](https://github.com/ollama/ollama/issues)\n",
    "\n",
    "### Learning More üéì\n",
    "- **LangChain + Ollama** tutorials\n",
    "- **RAG (Retrieval Augmented Generation)** guides\n",
    "- **Local AI development** best practices\n",
    "\n",
    "## üéØ Your Ollama Journey Starts Now!\n",
    "\n",
    "You now have everything needed to:\n",
    "- ‚úÖ Run AI models locally and privately\n",
    "- ‚úÖ Build intelligent applications  \n",
    "- ‚úÖ Create custom AI assistants\n",
    "- ‚úÖ Integrate AI into your workflows\n",
    "- ‚úÖ Scale for production use\n",
    "\n",
    "**Happy coding with Ollama! üöÄü§ñ**\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Pro Tip**: Bookmark this notebook for quick reference, and don't forget to experiment with different models and prompts to discover what works best for your specific use cases!\n",
    "\n",
    "*Created with ‚ù§Ô∏è for the Ollama community*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
